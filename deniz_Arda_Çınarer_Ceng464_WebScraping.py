# -*- coding: utf-8 -*-
"""WebScraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17khhF_DJe-iMpXEJj4ANNT66YlQgyifY
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q google-colab-selenium
import google_colab_selenium as gs
driver = gs.Chrome()

# Install selenium
!pip install selenium

# Set up Chrome options for Colab
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
#from selenium.webdriver.common.keys import Keys
#from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

# Set up Chrome options to use headless mode (for Colab)
chrome_options = Options()
chrome_options.add_argument("--headless")  # Run in headless mode
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--disable-gpu")

# Add extra options
chrome_options.add_argument("--window-size=1920,1080")  # Set the window size
chrome_options.add_argument("--disable-infobars")  # Disable the infobars
chrome_options.add_argument("--disable-popup-blocking")  # Disable pop-ups
chrome_options.add_argument("--ignore-certificate-errors")  # Ignore certificate errors
chrome_options.add_argument("--incognito")  # Use Chrome in incognito mode

# Set the path to chromedriver explicitly (installed by apt)
chrome_path = "/usr/bin/chromedriver"

# Initialize the WebDriver with the updated path
driver = gs.Chrome(options=chrome_options)

# ------------------------------------------------
# 1) ANA SAYFADAN KONU LİSTESİ ÇEK
# ------------------------------------------------
url = "https://discuss.huggingface.co/c/hub/23/l/hot"
driver.get(url)
time.sleep(5)

def scrape_titles_and_links():
    """
    Ana forum sayfasında (hub/hot), tüm konuları (başlık, link vb.) toplayıp
    (title, full_link, owner_nickname, reply_count, views_count, activity_data) şeklinde döndürür.
    """
    topics_data = []
    seen_topics = set()

    while True:
        # Konu başlıklarını (<a class="title.raw-link.raw-topic-link">) bul
        elements = driver.find_elements(
            By.CSS_SELECTOR,
            "span.link-top-line a.title.raw-link.raw-topic-link"
        )

        for elem in elements:
            # 1) Başlık ve Bağlantı
            title = elem.text.strip()
            full_link = elem.get_attribute("href")

            # 2) Topic Owner (Kullanıcı Adı)
            owner_elem = elem.find_element(
                By.XPATH,
                "./ancestor::tr//td[@class='posters topic-list-data']//a"
            )
            owner_nickname = owner_elem.get_attribute("data-user-card")

            # 3) Reply (Cevap) Sayısı
            reply_elem_list = elem.find_elements(
                By.XPATH,
                "./ancestor::tr//td[contains(@class,'num') "
                "and contains(@class,'posts-map') "
                "and contains(@class,'topic-list-data')]"
                "//span[@class='number']"
            )
            reply_count = reply_elem_list[0].text.strip() if reply_elem_list else "N/A"

            # 4) Views (Görüntülenme) Sayısı
            views_elem_list = elem.find_elements(
                By.XPATH,
                "./ancestor::tr//td[contains(@class,'num') "
                "and contains(@class,'views') "
                "and contains(@class,'topic-list-data')]"
                "//span[@class='number']"
            )
            views_count = views_elem_list[0].text.strip() if views_elem_list else "N/A"

            # 5) Activity (Son Gönderi Zamanı)
            activity_elem_list = elem.find_elements(
                By.XPATH,
                "./ancestor::tr//td[contains(@class,'num') "
                "and contains(@class,'topic-list-data') "
                "and contains(@class,'activity')]"
                "//span[@class='relative-date']"
            )
            activity_data = activity_elem_list[0].text.strip() if activity_elem_list else "N/A"

            # 6) Kaydı ekle
            unique_key = (title, full_link, owner_nickname, reply_count, views_count, activity_data)
            if unique_key not in seen_topics:
                topics_data.append(unique_key)
                seen_topics.add(unique_key)

        # Sonsuz kaydırma => sayfa sonuna git
        driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
        time.sleep(3)

        # Yeni element gelmiyorsa döngüden çık
        if len(elements) == len(driver.find_elements(
            By.CSS_SELECTOR, "span.link-top-line a.title.raw-link.raw-topic-link"
        )):
            break

    return topics_data

topics_data = scrape_titles_and_links()

print("Scraped Titles, Links, Owners, Replies, Views, and Activity:")
for i, (title, link, owner, reply_count, views_count, activity_data) in enumerate(topics_data, 1):
    print(f"{i}. Title:     {title}")
    print(f"   Link:      {link}")
    print(f"   Owner:     {owner}")
    print(f"   Replies:   {reply_count}")
    print(f"   Views:     {views_count}")
    print(f"   Activity:  {activity_data}\n")

# ------------------------------------------------
# 2) HER KONUYA GİDİP POST'LARI (USERNAME + TIMESTAMP + CONTENT) AL
# ------------------------------------------------
def scrape_topic_posts(driver, topic_url):
    """
    'topic_url' sayfasına girer, o konudaki her post'un
    - username
    - timestamp (span.relative-date)
    - content (div.cooked)
    verilerini bulur ve döndürür.
    """
    driver.get(topic_url)
    time.sleep(3)  # Konu sayfasının yüklenmesini bekle

    # Her post genelde 'div.topic-post' içinde yer alıyor (Discourse forum).
    post_divs = driver.find_elements(By.CSS_SELECTOR, "div.topic-post")

    post_data = []
    for post in post_divs:
        # 1) Username
        try:
            user_elem = post.find_element(
                By.CSS_SELECTOR,
                "div.names.trigger-user-card span.first.username a"
            )
            user_name = user_elem.text.strip()
        except:
            user_name = "N/A"

        # 2) Timestamp
        try:
            date_elem = post.find_element(By.CSS_SELECTOR, "div.post-info.post-date span.relative-date")
            date_text = date_elem.text.strip()             # Ekranda görünen tarih
            date_title = date_elem.get_attribute("title")
        except:
            date_text = "N/A"
            date_title = "N/A"

        # 3) Content: <div class="cooked"> metni
        try:
            content_elem = post.find_element(By.CSS_SELECTOR, "div.cooked")
            content_text = content_elem.text.strip()  # Etiketsiz, düz metin
        except:
            content_text = "N/A"

        # Her post verisini dict şeklinde ekleyelim
        post_data.append({
            "username": user_name,
            "date_text": date_text,
            "date_title": date_title,
            "content": content_text
        })

    return post_data

print("Now scraping Timestamps + Topic Content + Username from each topic page...\n")

all_topic_details = []
for idx, (title, link, owner, reply_count, views_count, activity_data) in enumerate(topics_data, 1):
    print(f"({idx}) Going to topic page: {link}")
    post_info_list = scrape_topic_posts(driver, link)

    topic_detail = {
        "title": title,
        "link": link,
        "owner": owner,
        "reply_count": reply_count,
        "views_count": views_count,
        "activity_data": activity_data,
        "posts": post_info_list   # her post => {"username": "...", "date_text": "...", "date_title": "...", "content": "..."}
    }
    all_topic_details.append(topic_detail)

# ------------------------------------------------
# 3) SONUÇLARI YAZDIR
# ------------------------------------------------
print("\n===== FINAL: Detailed Timestamps + Content + Username per Topic =====\n")
for topic_info in all_topic_details:
    print("Topic Title :", topic_info["title"])
    print("Link        :", topic_info["link"])
    print("Owner       :", topic_info["owner"])
    print("Replies     :", topic_info["reply_count"])
    print("Views       :", topic_info["views_count"])
    print("Activity    :", topic_info["activity_data"])
    print("Posts       :")
    for post in topic_info["posts"]:
        print(f"  - Username   : {post['username']}")
        print(f"    Post date  : {post['date_text']} (full={post['date_title']})")
        print(f"    Content    : {post['content'][:200]}...")
    print("")

driver.quit()

import csv

csv_filename = "forum_data.csv"

with open(csv_filename, "w", encoding="utf-8", newline="") as f:
    writer = csv.writer(f)

    # CSV'de sütun başlıklarımız:
    header = [
        "topic_title",
        "topic_link",
        "topic_owner",
        "reply_count",
        "views_count",
        "activity_data",
        "post_username",
        "post_date_text",
        "post_date_title",
        "post_content",
    ]
    writer.writerow(header)

    # Her topic içindeki her post'u döngüye alıp satır satır yazalım
    for topic in all_topic_details:
        for post in topic["posts"]:
            row = [
                topic["title"],
                topic["link"],
                topic["owner"],
                topic["reply_count"],
                topic["views_count"],
                topic["activity_data"],
                post["username"],
                post["date_text"],
                post["date_title"],
                # içerikteki \n karakterlerini tek satıra indiriyoruz
                post["content"].replace("\n", " "),
            ]
            writer.writerow(row)

print("Veriler CSV formatında kaydedildi:", csv_filename)